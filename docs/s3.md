# AWS S3 Provider

The S3 provider enables downloading datasets from Amazon Web Services (AWS) S3 buckets, including both private buckets (with credentials) and public buckets (no authentication required).

## What is AWS S3?

Amazon S3 (Simple Storage Service) is a cloud object storage service widely used for storing and distributing data. Many research organizations, public datasets, and cloud services use S3 for hosting large-scale datasets.

## Installation

### Install Repo2Data

```bash
pip install repo2data
```

### Install AWS CLI

The S3 provider requires the AWS Command Line Interface:

**Using pip:**
```bash
pip install awscli
```

**Using conda:**
```bash
conda install -c conda-forge awscli
```

**Official installers:**
Visit [AWS CLI Installation Guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)

**Verify installation:**
```bash
aws --version
```

## How It Works

The S3 provider:

1. **Detects S3 URLs** starting with `s3://`
2. **Uses AWS CLI `s3 sync`** to download data
3. **Supports public buckets** with `--no-sign-request` flag
4. **Supports authenticated access** for private buckets
5. **Syncs efficiently** (only downloads new/changed files)
6. **Preserves directory structure** from S3

## URL Format

S3 URLs follow this pattern:

```
s3://bucket-name/path/to/data
```

**Examples:**
- `s3://openneuro.org/ds000001` - Public neuroimaging dataset
- `s3://my-research-bucket/experiments/2024` - Private bucket path
- `s3://public-datasets/genomics/reference` - Public reference data

## Configuration

### Basic Configuration - Public Bucket

```yaml
data:
  src: "s3://openneuro.org/ds000001"
  dst: "./data"
  projectName: "openneuro_ds001"
```

No AWS credentials needed for public buckets!

### Private Bucket (Requires Credentials)

```yaml
data:
  src: "s3://my-private-bucket/research-data"
  dst: "./data"
  projectName: "private_research_data"
```

AWS credentials must be configured (see Authentication section below).

### Required Fields

- **`src`**: S3 URL (s3://bucket/path)
- **`dst`**: Destination directory
- **`projectName`**: Name for the dataset

### Optional Fields

- **`version`**: Version identifier for cache management

## Examples

### Example 1: Public OpenNeuro Dataset

Download brain imaging data from OpenNeuro's S3 bucket:

```yaml
data:
  src: "s3://openneuro.org/ds000005"
  dst: "./data"
  projectName: "openneuro_ds000005"
```

**Command line:**

```bash
repo2data myst.yml
```

OpenNeuro hosts public neuroimaging datasets on S3.

### Example 2: Specific S3 Path

Download from a specific path within a bucket:

```yaml
data:
  src: "s3://public-datasets/genomics/reference-genomes/hg38"
  dst: "./data"
  projectName: "hg38_reference"
```

### Example 3: Multiple S3 Datasets

```yaml
data:
  dataset1:
    src: "s3://openneuro.org/ds000001"
    dst: "./data"
    projectName: "ds000001"

  dataset2:
    src: "s3://openneuro.org/ds000002"
    dst: "./data"
    projectName: "ds000002"

  reference:
    src: "s3://public-datasets/reference/atlas"
    dst: "./data"
    projectName: "reference_atlas"
```

### Example 4: Private Research Bucket

```yaml
data:
  src: "s3://my-lab-bucket/experiments/experiment_2024"
  dst: "./data"
  projectName: "lab_experiment_2024"
```

Requires AWS credentials configured (see below).

### Example 5: Using from Jupyter Notebook

```python
from repo2data import Repo2Data
from repo2data.utils import locate_evidence_data
import nibabel as nib

# Download from S3 (skip if cached)
r2d = Repo2Data("myst.yml")
r2d.install()

# Locate the synced data
data_path = locate_evidence_data("openneuro_ds000005")

# Access neuroimaging files
anat_files = list(data_path.glob("**/anat/*.nii.gz"))
print(f"Found {len(anat_files)} anatomical scans")

# Load first scan
img = nib.load(str(anat_files[0]))
print(f"Image shape: {img.shape}")
```

### Example 6: Versioned S3 Data

Track different versions of your S3 data:

```yaml
data:
  src: "s3://research-bucket/analysis-data"
  dst: "./data"
  projectName: "analysis_data"
  version: "2024-01-15"  # Update version to trigger re-download
```

### Example 7: Processing Pipeline

```python
# In notebook: repo/content/analysis/preprocessing.ipynb
from repo2data.utils import locate_evidence_data
from pathlib import Path
import json

# Automatically find S3-synced data
data_path = locate_evidence_data("openneuro_ds000005")

# Access BIDS dataset structure
participants_file = data_path / "participants.tsv"
dataset_desc = data_path / "dataset_description.json"

# Read metadata
with open(dataset_desc) as f:
    metadata = json.load(f)
    print(f"Dataset: {metadata['Name']}")
    print(f"Version: {metadata.get('DatasetDOI', 'N/A')}")

# Process subjects
subject_dirs = [d for d in data_path.iterdir() if d.is_dir() and d.name.startswith('sub-')]
print(f"Found {len(subject_dirs)} subjects")
```

## Authentication

### Public Buckets

No authentication needed! The provider uses `--no-sign-request` flag:

```bash
aws s3 sync --no-sign-request s3://public-bucket/path ./destination
```

### Private Buckets

Configure AWS credentials using one of these methods:

#### Method 1: AWS Configure (Recommended)

```bash
aws configure
```

Enter:
- AWS Access Key ID
- AWS Secret Access Key
- Default region (e.g., `us-east-1`)
- Default output format (e.g., `json`)

Credentials stored in `~/.aws/credentials`

#### Method 2: Environment Variables

```bash
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_DEFAULT_REGION="us-east-1"
```

#### Method 3: IAM Role (EC2/Lambda)

If running on AWS infrastructure, use IAM roles (no manual credentials needed).

#### Method 4: AWS Profile

For multiple AWS accounts:

```bash
# Configure named profile
aws configure --profile research

# Use with Repo2Data by setting environment variable
export AWS_PROFILE=research
repo2data myst.yml
```

## S3 Sync Behavior

### Efficient Syncing

`aws s3 sync` only downloads:
- New files not present locally
- Files that have changed (based on size/timestamp)

This makes subsequent runs very fast!

### Directory Structure

S3 path structure is preserved:

```
s3://bucket/path/to/data/
  ├── file1.txt
  ├── folder1/
  │   └── file2.txt
  └── folder2/
      └── file3.txt

→ {dst}/{projectName}/
  ├── file1.txt
  ├── folder1/
  │   └── file2.txt
  └── folder2/
      └── file3.txt
```

### Progress Display

AWS CLI shows progress for each file:
- File name
- Transfer status
- Size information

## Common Use Cases

### 1. OpenNeuro Datasets

Access public neuroimaging datasets:

```yaml
data:
  src: "s3://openneuro.org/ds003097"
  dst: "./data"
  projectName: "naturalistic_viewing"
```

Browse datasets at [openneuro.org](https://openneuro.org)

### 2. Public Reference Data

Download reference genomes, atlases, etc.:

```yaml
data:
  src: "s3://1000genomes/release/20130502"
  dst: "./data"
  projectName: "1000genomes_ref"
```

### 3. Lab Private Data

Access your lab's S3 storage:

```yaml
data:
  src: "s3://lab-research-bucket/projects/experiment_a"
  dst: "./data"
  projectName: "experiment_a"
```

### 4. Cloud-Based Workflows

Integrate with cloud computing:

```yaml
data:
  input_data:
    src: "s3://analysis-bucket/inputs"
    dst: "./data"
    projectName: "analysis_inputs"

  reference_data:
    src: "s3://reference-bucket/atlas"
    dst: "./data"
    projectName: "reference_atlas"
```

### 5. Large-Scale Datasets

Efficiently download large datasets:

```yaml
data:
  src: "s3://big-data-bucket/imaging/complete-dataset"
  dst: "./data"
  projectName: "complete_imaging_dataset"
  version: "v1.0"
```

S3 sync handles large data efficiently.

## Advantages of S3 Provider

1. **Efficient Syncing**: Only downloads new/changed files
2. **Public Access**: No credentials needed for public buckets
3. **Scalable**: Handles datasets of any size
4. **Cloud Integration**: Works with AWS ecosystem
5. **Structure Preservation**: Maintains directory hierarchy
6. **Incremental Updates**: Re-running is fast (only syncs changes)

## Troubleshooting

### AWS CLI Not Installed

**Error**: `FileNotFoundError: AWS CLI is not installed`

**Solution**: Install AWS CLI:

```bash
pip install awscli
# or
conda install -c conda-forge awscli
```

### Access Denied (Private Bucket)

**Error**: `AWS S3 sync failed` - Access Denied

**Solution**:
- Configure AWS credentials: `aws configure`
- Verify credentials have access to the bucket
- Check bucket permissions/policies
- Ensure IAM user has `s3:ListBucket` and `s3:GetObject` permissions

### Bucket Does Not Exist

**Error**: Bucket not found

**Solution**:
- Verify bucket name is correct (case-sensitive)
- Ensure you have permission to access bucket
- Check if bucket is in a specific region
- Public buckets may require `--no-sign-request`

### Region Error

**Error**: Bucket is in a different region

**Solution**:
- Set the correct region: `aws configure set region us-west-2`
- Or use environment variable: `export AWS_DEFAULT_REGION=us-west-2`

### Sync Failed

**Error**: `AWS S3 sync failed with return code X`

**Solution**:
- Check internet connection
- Verify AWS CLI is up to date: `pip install --upgrade awscli`
- Check AWS status: [status.aws.amazon.com](https://status.aws.amazon.com/)
- Review error message in stderr for specific issue

### Slow Download

**Problem**: Download is very slow

**Solution**:
- S3 transfer speeds depend on location and bandwidth
- Consider using AWS region closer to you
- Check if bucket has transfer acceleration enabled
- Network issues on your end

## Best Practices

1. **Use public URLs when available**:
   ```yaml
   src: "s3://openneuro.org/ds000001"  # Public - no credentials
   ```

2. **Add version tags** for tracking:
   ```yaml
   version: "v2.1"
   ```

3. **Secure credentials properly**:
   - Never commit credentials to git
   - Use IAM roles when possible
   - Rotate access keys regularly
   - Use AWS Secrets Manager for production

4. **Document bucket regions**:
   ```yaml
   # data:
   #   src: s3://my-bucket/data
   #   # Bucket region: us-east-1
   ```

5. **Use descriptive project names**:
   ```yaml
   projectName: "openneuro_ds000001"  # Clear
   projectName: "s3data1"  # Avoid
   ```

6. **Check bucket policies** for public buckets:
   - Ensure bucket allows public access
   - Verify `--no-sign-request` works

## AWS S3 Costs

**Public buckets**: Usually free to download (paid by bucket owner)

**Private buckets**:
- Data transfer out charges apply
- Check [AWS S3 Pricing](https://aws.amazon.com/s3/pricing/)
- Consider AWS region selection

**Best practice**: Use AWS cost monitoring tools

## Public S3 Datasets

Many organizations host public datasets on S3:

- **OpenNeuro**: `s3://openneuro.org/` - Neuroimaging datasets
- **1000 Genomes**: `s3://1000genomes/` - Genomics data
- **AWS Public Datasets**: [registry.opendata.aws](https://registry.opendata.aws/)
- **NIH STRIDES**: Various biomedical datasets

## S3 URI vs HTTP

S3 buckets can be accessed via:
- **S3 URI**: `s3://bucket/path` - Use S3 provider (this doc)
- **HTTP URL**: `https://bucket.s3.amazonaws.com/path` - Use HTTP provider

Prefer S3 URI for:
- Efficient syncing
- Large datasets
- Directory structures
- Private buckets

## Advanced: S3 Sync Options

The provider uses:
```bash
aws s3 sync --no-sign-request s3://bucket/path destination
```

For private buckets (credentials configured):
```bash
aws s3 sync s3://bucket/path destination
```

Future enhancements could support additional `aws s3 sync` options.

## Security Considerations

1. **Public buckets**: Safe to access without credentials
2. **Private buckets**:
   - Protect AWS credentials
   - Use least-privilege IAM policies
   - Enable CloudTrail logging
   - Monitor access patterns

3. **Never commit credentials** to version control

## Related Resources

- [AWS CLI Documentation](https://docs.aws.amazon.com/cli/)
- [AWS S3 User Guide](https://docs.aws.amazon.com/s3/)
- [AWS Public Datasets](https://registry.opendata.aws/)
- [OpenNeuro](https://openneuro.org/)

## Back to Main Guide

Return to the [main documentation](./data.md) for general Repo2Data features and other providers.
